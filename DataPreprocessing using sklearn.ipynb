{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing using sklearn\n",
    "We are going to be performing tokenization (Parsing words) and feature extraction(vectorization). The sklearn library offers tools to perform these two operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words model\n",
    "Machine Learning algorithms take vectors of numbers as input and hence our raw text needs to be converted into fixed-length vectors of numbers. One method to achieve this is using the Bag-of-words model. We assign each word in the sentence a unique number. Then any corpus of text can be encoded as a fixed-length vector with the length of the vocabulary of known words. Then the vectorizer returns the count of each word in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beijing': 1, 'in': 3, 'asia': 0, 'is': 4, 'china': 2}\n"
     ]
    }
   ],
   "source": [
    "text=[\"Beijing is in china. China is in Asia\"]\n",
    "vectorizer=CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2 1 2]]\n"
     ]
    }
   ],
   "source": [
    "text2=[\"China is India's neighbour and also in Asia. China is the major exporter of zirconium\"]\n",
    "vector2=vectorizer.transform(text2)\n",
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see from the above example that an array containing the count of each word is returned by the vectorizer. Which gives us an idea of the frequency of the words in a given corpus of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies with TfidfVectorize\n",
    "### Term Frequency- Inverse Document Frequency \n",
    "\n",
    "The countVectorizer model just gives us the count of each word in the document is is not very meaningful because some words like the will appear many times.\n",
    "So, we use TFidF model which has two main components\n",
    "\n",
    "1. Term Frequency: This summarizes how often a given word appears in the document\n",
    "2. Inverse Document Frequency: This downscales words that appear a lot across documents\n",
    "\n",
    "TFIDF are word frequency scores that try to highlight words that are more interesting . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'dog': 1, 'fox': 2, 'lazy': 4, 'over': 5, 'quick': 6, 'jumped': 3, 'brown': 0}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text=['The quick brown fox jumped over the lazy dog',\"The dog\",\"The fox\"]\n",
    "vectorizer=TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(text)\n",
    "#Summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "#Encode document\n",
    "vector=vectorizer.transform([text[0]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a vocubalary of 8 words is learned from the document and each word is assigned a unique integer index in the output vector. The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: the at index 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing with HashingVectorizer\n",
    "\n",
    "In this method we perform one way hashing of words and convert them to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "text=[\"The quick brown fox jumped over the lazy dog\"]\n",
    "vectorizer=HashingVectorizer(n_features=20)\n",
    "\n",
    "vector=vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
