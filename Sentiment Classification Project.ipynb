{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "from os import listdir\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Document into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    #Open the file as read only\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    re_punc=re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    #remove non-alphabetic values\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    #Stop Words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if w not in stop_words]\n",
    "    #Filter our short tokens\n",
    "    tokens=[word for word in tokens if len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc, clean and return line of tokens\n",
    "def doc_to_line(filename,vocab):\n",
    "    doc=load_doc(filename)\n",
    "    #clean doc\n",
    "    tokens=clean_doc(doc)\n",
    "    #filter by vocab\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines=list()\n",
    "    for filename in listdir(directory):\n",
    "        #Skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path=directory+'/'+filename\n",
    "        #load and clean the doc\n",
    "        line=doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_dataset(vocab, is_train):\n",
    "    #Laod documents\n",
    "    neg=process_docs('./txt_sentoken/neg/', vocab, is_train)\n",
    "    pos=process_docs('./txt_sentoken/pos/', vocab,is_train)\n",
    "    docs=neg+pos\n",
    "    #prepare labels\n",
    "    labels=[0 for _ in range(len(neg))]+[1 for _ in range(len(pos))]\n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    #Define the network\n",
    "    model=Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores=list()\n",
    "    n_repeats=10\n",
    "    n_words=Xtest.shape[1]\n",
    "     \n",
    "    for i in range(n_words):\n",
    "        #Define the network\n",
    "        model=define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        #evaluate\n",
    "        _, acc=model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s'%((i+1), acc))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    #create the tokenizer\n",
    "    tokenizer=Tokenizer()\n",
    "    #fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    #encode training dataset\n",
    "    Xtrain=tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    #encode the testing dataset\n",
    "    Xtest=tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filename='vocab.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n",
    "#load all the reviews\n",
    "train_docs, ytrain=load_clean_dataset(vocab, True)\n",
    "tests_docs, ytest=load_clean_dataset(vocab, False)\n",
    "#run the experiment\n",
    "modes=['binary','count','tfidf','freq']\n",
    "results=DataFrame()\n",
    "for mode in modes:\n",
    "    #Prepare data for mode\n",
    "    Xtrain, Xtest=prepare_data(train_docs, tests_docs, mode)\n",
    "    #evaluate the model on data for mode\n",
    "    results[mode]=evaluate_model(Xtrain, ytrain, Xtest, ytest)\n",
    "print(results.describe())\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
